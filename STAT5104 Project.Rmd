---
title: STAT5104 Data Mining Project
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage{float}
author:
  - CHAN Yiu Fung (1155010561)
  - CHUNG Wai Tung (1155118014)
  - LAM Siu Hung (1006201460)
  - LAU Chiu Kit (1155120306)
  - WONG Tsz Wing (1004666311)
  - WONG Yiu Chung (1155017920)
date: <center>`r format(Sys.time(), '%d %B, %Y')`</center>
abstract: |
  This report explores the possibility of predicting human movement using spatial data of exercises. Seven data mining models are used to extract latent pattern from the dataset. The present study is successful in predicting various types of human physical movements with extremely high accuracy.  
  
output:
  pdf_document:
    fig_caption: true
    number_sections: true
  html_document:
    keep_md: no
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
  word_document: default
bibliography: r-references.bibtex
csl: apa.csl  
nocite: '@*'
--- 

\newpage
\tableofcontents
\newpage
```{r, echo=FALSE, results='hide'}
remove(list=ls()); gc();
```

```{r setup, include=TRUE, echo = FALSE, results = 'hide', message=FALSE, warning=FALSE}
#Prepare environment 
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, results = TRUE, message=FALSE, warning=FALSE);
```

```{r libraries}
check.packages <- function(pkg)
{
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])];
    if (length(new.pkg)) 
    {
      install.packages(new.pkg, dependencies = TRUE);
      sapply(pkg, require, character.only = TRUE);
    }
}

# packeges required by project
packages<-c("caret",
            "rpart",
            "e1071",
            "klaR",
            "rattle",
            "doParallel",
            "parallel",
            "randomForest",
            "gbm",
            "MLmetrics",
            "dplyr",
            "ggplot2",
            "GGally",
            "lattice");
check.packages(packages);

library(dplyr);
library(caret);
```

```{r}
set.seed(5104);
```


# Introduction
The research topic, Human Activity Recognition (HAR), is becoming more and more popular among the computing research community. In the traditional HAR research, researchers mainly focused on predicting what activity a person was performing at a specific point of time. Meanwhile, latest researchers have shifted the focus on ¡§how well¡¨ the activities have been performed.  In real-life, we can apply the ideas, for example, in sports training. 
In this report, we explored the Weight Lifting Exercises Dataset [@velloso2013qualitative] and attempted to assess if the participants performed the specific weight lifting exercise, Unilateral Dumbbell Biceps Curl (hereafter refers to ¡§the exercise¡¨), correctly from the data collected via various sensors attached on different parts of the body, which includes arm, belt, forearm, and dumbbell.  The type of mistakes in the exercise can also be identified.
Six male participants aged between 20-28 years were asked to wear a number of body sensors to perform one set of 10 repetitions of the exercise. Based on the sensor data collected, we can trace the outcome of the performance accordingly.  The performance outcome can be grouped into five classes, one corresponding to the specified execution of the exercise, while the other 4 classes corresponding to some common mistakes. Each sensor generated a set of readings in three dimensions.


# The Data

The data for this project come from [this source](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). 

The dataset contains 160 variables, which include one target variable ¡§Classe¡¨ and 159 readings from the sensors.  Each ¡§Classe¡¨ represents a specific performance outcome. This dataset is unique in a way that while there are many variables, each are fundamentally the same, i.e. each set of three columns represents a sensor attached on different parts of the body.  Each sensor generates data according to its rotation around a spatial axis, giving spatial data on three dimensions. Hence all 159 columns of data are highly similar to each other. 

The target variable classes is defined as below: 

<ol type="a">
  <li>Class A: exactly according to the specification (i.e. performing the exercise correctly);</li>
  <li>Class B: throwing the elbows to the front; </li>
  <li>Class C: lifting the dumbbell only halfway;</li>
  <li>Class D: lowering the dumbbell only halfway; and</li>
  <li>Class E: throwing the hips to the front.</li>
</ol>

## Data preparation

Because of the following:

* Predictor with well-defined meaning
* Similar scale
* Similar range
* All continuous

scaling / standardising may not yield the best result since this may cause distortion. Data are not rescaled or normalised in this report.

### Load data
```{r get data}
dataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
data <- read.csv(dataURL, header = TRUE);
```

### Data cleaning 
The data are further processed by:  
* Removing the first seven fields which are just descriptive data  
* Removing near zero variance fields  
* Removing columns with more than 10% missing values

```{r data cleaning}
#Remove the first seven columns
data <- data[,-(1:7)];

#Remove NearZeroVariance variables
nzv <- nearZeroVar(data, saveMetrics=TRUE);
data <- data[,nzv$nzv == FALSE];

#Clean variables with mostly NA
dataNA <- apply(data, 2, function(col){sum(is.na(col))/length(col)});
data <- data[,which(dataNA < .1)];
```
`r ncol(data)` columns remains in the dataset post-cleaning. The following table lists the remaining variables 

```{r str data, results='markup'}
str(data)
```

### Slicing into training and testing sets
The training data set is sliced into 80% for training and 20% for testing.
```{r data splitting}
inTrain <- createDataPartition(y=data$classe, p=0.80, list=FALSE); #Data slicing
train <- data[inTrain,];
test <- data[-inTrain,];
```

### Overview of cleaned dataset
```{r overview of data}
dim(train);
```


## Principal Component Analysis

Principal Component Analysis (PCA) is a dimension reduction technique. A reduced dataset allows faster processing and smaller storage. In the context of data mining, PCA reduce the number of variables to be used in a model by focusing only on the components accounting for the majority of the variance. Highly correlated variables are also removed as a result of PCA.

```{r PCA}
prComp <- caret::preProcess(train[,-length(train)], method = "pca", thresh = 0.99);
trainPC <- predict(prComp, train[,1:ncol(train)-1]);
trainPC$classe <- train$classe;
testPC <- predict(prComp, test[,1:ncol(test)-1]);
testPC$classe <- test$classe;
```
Here, PCA is able to reduce the dimension of the datasets from `r ncol(data) - 1` to `r ncol(trainPC)` while retaining 99% of the information. This reduces model complexity and improves scalability. 

As a side note, PCA is usually performed on scaled  or normalised dataset to prevent the resulting principle sub-space from being dominated by variables with large scales. As mentioned above, because the variables in the dataset are of similar nature, scaling or normalised provides little added benefits. Hence such procedures are not used. 


```{r, echo=FALSE, fig.show='hide'}
gpairs_lower <- function(g)
{
    g$plots <- g$plots[-(1:g$nrow)];
    g$yAxisLabels <- g$yAxisLabels[-1];
    g$nrow <- g$nrow - 1;
    
    g$plots <- g$plots[-(seq(g$ncol, length(g$plots), by = g$ncol))];
    g$xAxisLabels <- g$xAxisLabels[-g$ncol];
    g$ncol <- g$ncol - 1;
    
    return(g);
}

g <- GGally::ggpairs(trainPC[, c(1:4)],
                     upper  = list(continuous = "blank"),
                     diag  = list(continuous = "blankDiag"),
                     lower  = list(continuous = "points",
                                   mapping = ggplot2::aes(colour = trainPC$classe)
                                   )
                     );
gpairs_lower(g)            
```

# Methods

## Learning Models
Seven learning methods are adopted in this report. Namely: 

1. Decision Tree;
1. K-Nearest Neighbour;
1. Multinomial Logistic Regression;
1. Naive Bayes;
1. Neuro Network;
1. Random Forest;
1. Tree Bagging.  

The methods can be classified as eager learner (Decision Tree, Tree Bagging, Random Forest, and Neuro Network) and lazy learner (K-Nearest Neighbour and Naive Bayes). The library `Caret` is used to generate training models .


## Resampling: Cross Validation

Cross Validation is performed on each training methods to infer model performance.

### Choosing between LOOCV and K-Fold

Leave-One-Out Cross-Validation (LOOCV) and K-Fold are common resampling methods for accessing model performance. While LOOCV estimates test error with lowest bias (averaging validation errors across n models), K-Fold CV is much less computationally intensive. Yet there is another advantage to using K-fold CV. This has to do with a bias-variance trade-off.

Estimates produced by LOOCV is plagued by high variance compared to that produced by K-fold CV. This is because each statistics (accuracy, AUC, F1, log loss etc) in LOOCV are produced by models trained on virtually identical datasets. The final averaged statistic is an average of statistics from n models which are highly positively correlated. On the other hand, K-fold CV outputs K (which is usually much less than n) statistics which are less correlated as there are less overlap among models. The average of strongly correlated quantities has higher variance than the average of weakly correlated quantities; hence the estimated statistics from LOOCV tends to have higher variance that that from K-fold. 

The dataset in the report consists of relatively large number of observations (`r ncol(trainPC)` rows). Hence a 10 fold cross-validation is performed.

### Performance Measures for Multi-Class Problems

The following are some of many viable model performance metrics for choosing the best model out of the many models `caret::train` create using different parameters. For example, `caret::train` tries different $k$ in KNN. 

* Accuracy and Kappa
* Area Under ROC Curve
* F1
* Logarithmic Loss

```{r model trainControl}
tc <- caret::trainControl(method = "cv", #resampling method = cross validation
                          number = 10,   #10-fold validation
                          classProbs = TRUE,
                          summaryFunction = multiClassSummary,
                          verboseIter=FALSE,
                          allowParallel=TRUE);

metric <- "logLoss";
```

```{r parallel processing}
#Parallel Processing, leaves you one core for other stuff.
#Plz try not to do CPU intensive tasks while modelling.
cl <- parallel::makeCluster(parallel::detectCores()- 1);
doParallel::registerDoParallel(cl);
```

## Lazy Learners

Lazy learners simply store the training data without performing further munging, until a test dataset is presented. During model training, Lazy Learners require significantly less computational operation as there is no new algorithm being developed; for the same reason, Lazy Learners are slow when used for prediction because new data are used to compute predictions instead of relying on a pre-calculated algorithm.

Naive Bayes and K-Nearest-Neighbour (KNN) are used in this section. Both lazy learners are expected to perform quickly on large datasets like the one used in this report. KNN relies heavily on Euclidean distance (L2 norm) between observations and is more appropriate on scaled or normalised data. Hence, this model is expected to perform less well than other data mining models used in this report.  
```{r lazy learners}
if (file.exists("nb.rds"))
{
        nb <- readRDS(file = "nb.rds");
        nbTime <- readRDS(file = "nbTime.rds");
} else
{
        nbTime <- system.time(
          nb <- caret::train(classe ~ .,        #naive bayes
                             data = trainPC,
                             method = "nb",
                             metric = metric, 
                             trControl= tc)
          );       
        saveRDS(nb, file = "nb.rds");
        saveRDS(nbTime, file = "nbTime.rds");
}

if (file.exists("knn.rds"))
{
        knn <- readRDS(file = "knn.rds"); 
        knnTime <- readRDS(file = "knnTime.rds"); 
} else
{
        knnTime <- system.time(
          knn <- caret::train(classe ~ .,       #knn
                              data = trainPC,
                              method = "knn",
                              metric = metric,
                              trControl= tc)
          );     
        saveRDS(knn, file = "knn.rds");
        saveRDS(knnTime, file = "knnTime.rds");
}
```

## Multinomial logistic regression

```{r multinomial logistic regression}

if (file.exists("multinom.rds"))
{
        multinom <- readRDS(file = "multinom.rds");
        multinomTime <- readRDS(file = "multinomTime.rds");
} else
{
        multinomTime <- system.time(
          multinom <- caret::train(classe ~ .,          #Multinomial Logistic Regression
                                   data = trainPC,
                                   method = "multinom",
                                   metric = metric,
                                   trControl= tc)
          );
        saveRDS(multinom, file = "multinom.rds");
        saveRDS(multinomTime, file = "multinomTime.rds");
}
```

## Tree based models

Tree-based methods tend to perform well on unprocessed data (i.e. without normalizing, centering, scaling features).

Decision Trees often produce predictions with low bias but high variance. The more complex the tree, the more apparent this becomes (overfitting). Methods have been proposed to overcome this issue. This includes Bootstrap Aggregation (Bagging), as well as Random Forest. 

The idea behind tree bagging is to create many trees, each trained from bootstrapped data from the original dataset. Each tree is slightly different from each other because they are trained with mildly different datasets. Classification decision is then performed by popular vote across all trees. This method reduces variance by averaging decisions among many trees. There is a caveat though: tress turn out to be very similar to each other when there exists a (or few) extremely strong predictor, following by some moderately strong predictors.  Each tree will have similar node splitting because of these strong predictors, which renders each tree to have practicality the same decision rules. Unfortunately, as mentioned above, the variance of the averages of highly correlated quantities is also high. This means tree bagging provides little improvements in terms of variance reduction.

Random Forest enhances tree bagging through a tweak: at each node split, the algorithm randomly picks a subset of size $m$ predictors out of all $p$, then choose the best predictor for this node split as normally seen in decision trees. This way, each tree is more likely to be different from each other. And hence their averages are less varying. The choice of $m$ is often the square root of $p$ but other method of choosing $m$ also exists.

```{r tree_models}

if (file.exists("ctree.rds"))
{
        ctree <- readRDS(file = "ctree.rds");
        ctreeTime <- readRDS(file = "ctreeTime.rds");
} else
{
        ctreeTime <- system.time(
          ctree <- caret::train(classe ~ .,       #decision tree
                                data = trainPC,
                                method = "rpart",
                                metric = metric,
                                trControl= tc)
          );
        saveRDS(ctree, file = "ctree.rds");
        saveRDS(ctreeTime, file = "ctreeTime.rds");
}

if (file.exists("treebag.rds"))
{
        treebag <- readRDS(file = "treebag.rds");
        treebagTime <- readRDS(file = "treebagTime.rds");
} else
{
        treebagGrid <- expand.grid(.mtry = ncol(trainPC) - 1);
        treebagTime <- system.time(
          treebag <- caret::train(classe ~ .,      #Tree bagging
                                  data = trainPC,
                                  method = "rf",
                                  metric = metric,
                                  tuneGrid = treebagGrid,
                                  trControl= tc)
          );
        saveRDS(treebag, file = "treebag.rds");
        saveRDS(treebagTime, file = "treebagTime.rds");
}

if (file.exists("rf.rds"))
{
        rf <- readRDS(file = "rf.rds");
        rfTime <- readRDS(file = "rfTime.rds");
} else
{
        rfGrid <- expand.grid(.mtry = sqrt(ncol(trainPC) - 1));
        rfTime <- system.time(
          rf <- caret::train(classe ~ .,        #Random Forest
                             data = trainPC,
                             method = "rf",
                             metric = metric,
                             tuneGrid = rfGrid,
                             trControl= tc)
          );
        saveRDS(rf, file = "rf.rds");
        saveRDS(rfTime, file = "rfTime.rds");
}
```
Note that in the code above, both models `treebag` and `rf` employ the training method rf. This is because tree bagging is in fact a special case of Random Forest where $m$ = $p$. 

## Neuro-Net

R doesn't provide an easy way to model multilayer perceptron (Neuro Network). Hence a single-layer perceptron is modelled below. Neuro Networks tend to be scale invariant (just like tree based models): rescaling the input vector is equivalent to changing the weights and biases of the network, result in the exact same outputs as before.
```{r neuro_net}
if (file.exists("NN.rds"))
{
        NN <- readRDS(file = "NN.rds");
        NNTime <- readRDS(file = "NNTime.rds");
} else
{
        nnetGrid <-  expand.grid(
                size = seq(from = 1, to = 10, by = 1),
                decay = c(0.5, 0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7));
        NNTime <- system.time(
          NN <- caret::train(classe ~ .,
                             data = trainPC,
                             method = "nnet",
                             metric = metric,
                             tuneGrid = nnetGrid,
                             trControl= tc, verbose=FALSE));
        saveRDS(NN, file = "NN.rds");
        saveRDS(NNTime, file = "NNTime.rds");
}
```
The parameter `size` specifies the number of units in the hidden layer. Sizes ranging from 1 to 10 are experimented for best results.
The parameter `decay` specifies the regularisation of the number of nodes: model with high node counts are more heavily penalised

```{r stopCluster}
#parallel::stopCluster(cl);
```

## Compare Models
```{r}
results <- caret::resamples(list(NaiveBayes = nb,
                          KNearestNeighbor = knn,
                          MultinomialLogit = multinom,
                          DecisionTree = ctree,
                          TreeBagging = treebag,
                          RandomForest = rf,
                          NeuroNetwork = NN));

results$metrics
```
There are a total of `r length(results$metrics)` metrics for comparing models.

# Findings

## Comparing models

Averages of LogLoss, Accuracy, F1 and AUC are used to assess the performances of the models.

```{r result graphs}
summaryStat <- summary(results)$statistics;

scales <- list(x=list(relation="free"), y=list(relation="free"));
metrics <- c("Accuracy", "AUC", "logLoss", "Mean_F1");

lattice::dotplot(results, scales=scales,  metric=metrics, main="Model Performances");

times <- c(nbTime[[3]], knnTime[[3]],
           multinomTime[[3]],
           ctreeTime[[3]], treebagTime[[3]], rfTime[[3]],
           NNTime[[3]]) %>% round(., 3)
models <- c("Naive Bayes", "KNN",
            "Multi Nomial",
            "Decision Tree", "Tree Bagging", "Random Forest",
            "Neuro-Net")
elapsedTime <- data.frame(models = models, times = times) %>% arrange(times);


timePlot <- ggplot2::ggplot(data=elapsedTime, aes(x=reorder(models, times), y=times)) +
  ggplot2::geom_bar(stat="identity") +
  ggplot2::ggtitle("Training Time") +
  ggplot2::theme(plot.title = element_text(hjust = 0.5)) +
  ggplot2::xlab("Models") + 
  ggplot2::ylab("Time (seconds)") + 
  ggplot2::geom_text(ggplot2::aes(label=times), vjust=-.5);
timePlot
```

Among the seven models, Random Forest, Tree Bagging and KNN outperform the other four models. Surprisingly, KNN appears to outperform all other models holistically: lowest log Loss value at `r summaryStat$logLoss["KNearestNeighbor","Mean"]`, highest Mean F1, Accuracy, and AUC at `r summaryStat$Mean_F1["KNearestNeighbor","Mean"]`, `r summaryStat$Accuracy["KNearestNeighbor","Mean"]`, `r summaryStat$AUC["KNearestNeighbor","Mean"]` respectively. In addition, KNN has the second lowest learning time (which is less surprising given its lazy learning nature) at `knnTime[[3]]` seconds (wall clock), beaten by Decision Tree only.

The training data used in the report remain at their original scale. KNN is supposed to suffer from  neighbors being aligned along the direction of the axis with the smaller range. This somewhat reaffirms the notion of the dataset having variables with similar scales and ranges. 

KNN performs well at various metrics, as well as having a low training time. Thus, KNN is chosen as the final model to be tested.


#Model Performance
```{r confusion matrix, results='markup'}
confusion <- confusionMatrix(predict(knn, testPC), testPC$classe);
confusion
```
`KNN` is able to predict future data with `r confusion$overall[[1]]` accuracy and `r confusion$overall[[2]]` Kappa. 

\newpage
Models in this report were built on machine with the following specs

* Processor: Intel Xeon(R) CPU E5-2603 v4 @ 1.70GHz x 12
* Memory: 31.3 GiB
* Graphics: NVS 315/PCIe/SSE2
* OS type: 64-bit
* OS: ubuntu 16.04 LTS

\newpage
# References